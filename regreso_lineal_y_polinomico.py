# -*- coding: utf-8 -*-
"""Regreso_lineal_y_polinomico

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bBFxKN4413iYWc2evtrLCn-AJQFEZlvG

# **Modelos Machine Learning**

Desarrollo de dos modelos de machine learning, un regresor lineal multiparamétrico y un regresor polinómico los cuales tienen como objetivo resolver un problema de regresión del dataset Synchronous Machine Dataset.

# **Modulos de importación**
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import time
import math
import copy
import scipy as sp
import numpy as np
import pandas as pd
from matplotlib import rcParams
import seaborn as sns #; sns.set()
from matplotlib import pyplot as plt

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score

"""# **Synchronous Machine Dataset**

> **Descripción del Dataset:** Los motores síncronos (SM) son motores de CA con velocidad constante. Se obtiene un conjunto de datos SM a partir de un conjunto experimental real. La tarea es crear modelos sólidos para estimar la corriente de excitación de SM. Tomado de:https://archive.ics.uci.edu/dataset/607/synchronous+machine+data+set

> **Información de Atributos:**

\begin{array}{|c|c|} \hline
  \textbf{Variable} & \textbf{Nombre} \\
  \hline
  \text{Iy} & \text{Corriente de carga}  \\
  \hline
  \text{PF} & \text{Factor de potencia} \\
  \hline
  \text{e} & \text{Error de factor de potencia} \\
  \hline
  \text{dIf} & \text{Cambio de corriente de excitación de máquina síncrona} \\
  \hline
  \text{If} & \text{Corriente de excitación de la máquina síncrona} \\
  \hline
\end{array}

> **Licencia:** Este conjunto de datos tiene una licencia Creative Commons Attribution 4.0 International (CC BY 4.0). Esto permite compartir y adaptar los conjuntos de datos para cualquier propósito, siempre que se otorgue el crédito apropiado.

# **Importar Dataset**
"""

#@title **Visualización**
RUTA = ''
df = pd.read_csv(RUTA, delimiter=';')
df.columns = ["IY","PF","e","dIf","If"]
df

df.describe()

"""# **Análisis exploratorio de datos**"""

#@title **Análisis exploratorio de datos mediante Pandas Profiling**
! pip install -q https://github.com/pandas-profiling/pandas-profiling/archive/master.zip
! pip install -U pandas-profiling[notebook] -q
! jupyter nbextension enable --py widgetsnbextension
from pandas_profiling import ProfileReport

profile = ProfileReport(df, title='Análisis exploratorio de datos mediante Pandas Profiling', html={'style':{'full_width':True}})
profile.to_notebook_iframe()

#@title **Valores Nulos**
df.isnull().sum()

#@title **Correlación**
fig, ax = plt.subplots(figsize=(10,8))
correlation  = df.corr(method = 'pearson')
sns.heatmap(correlation, annot=True, cmap='Blues', fmt=".3f")

#@title **Gráfico de pares**
sns.pairplot(df,size=1.8)

#@title **Selección de 3 características**
data = np.array(df, dtype=float)
y_ = data[:,-1]
x_ = [data[:,1], data[:,2], data[:,3]]
ly = 'If'
lx = ['PF','e','dIf']

plt.figure(figsize=(15, 15))
plt.suptitle("Selección de 3 características")
for i in range(3):
  ax = plt.subplot(3, 3, i + 1)
  plt.plot(x_[i], y_, 'b.')
  plt.xlabel(lx[i],fontsize=15)
  plt.ylabel(ly,fontsize=15)
  plt.grid()
plt.subplots_adjust(wspace=0.3, hspace=0.5)
plt.show()

#@title **Histograma**
plt.hist(df['If'])
plt.xlabel('If',fontsize=15)
plt.ylabel('Valores',fontsize=15)
plt.grid()
plt.show()

#@title **Gráfico de Densidad**
sns.displot(data=df['If'],  kde=True)

"""# **Datos atípicos**"""

df_cl = df.drop(columns=['IY'])
df_cl.head()

#@title **Datos Atípicos**
iqr = sp.stats.iqr(df_cl['If'])
print('IQR =',iqr)

lower_threshold = np.quantile(df_cl['If'], 0.25) - 1.5 * iqr
upper_threshold = np.quantile(df_cl['If'], 0.75) + 1.5 * iqr
outliers = df_cl[(df_cl['If'] < lower_threshold) | (df_cl['If'] > upper_threshold)]['If']

print('Umbral inferior =',lower_threshold,'\nUmbral superior =', upper_threshold)

plt.figure(figsize=(15,2))
plt.boxplot(df_cl['If'], vert=False)
plt.plot(outliers, [1]*len(outliers), 'bo', label='Valores atípicos')
plt.plot(upper_threshold,1,'rx', label='Límite valores atípicos') #added upper outlier
plt.plot(lower_threshold,1,'rx') #added lower outlier
plt.legend(loc='upper right')
plt.grid()
plt.show()

"""Imprimir outliers"""

# outliers = (consumption_by_country[(consumption_by_country > upper_threshold) | (consumption_by_country < lower_threshold)])
outliers = (df_cl['If'][(df_cl['If'] > upper_threshold) | (df_cl['If'] < lower_threshold)])
print(outliers)
print('Total de muestas:',len(outliers))

df_cl_filtrado = df_cl[(df_cl['If'] >= lower_threshold) & (df_cl['If'] <= upper_threshold)]
df_cl_filtrado.head()

#IQR
iqr = sp.stats.iqr(df_cl_filtrado['If'])
lower_threshold = np.quantile(df_cl_filtrado['If'], 0.25) - 1.5 * iqr
upper_threshold = np.quantile(df_cl_filtrado['If'], 0.75) + 1.5 * iqr
outliers = df_cl_filtrado[(df_cl_filtrado['If'] < lower_threshold) | (df_cl_filtrado['If'] > upper_threshold)]['If']

plt.figure(figsize=(15,2))
plt.boxplot(df_cl_filtrado['If'], vert=False)
plt.plot(outliers, [1]*len(outliers), 'bo', label='Valores atípicos')
plt.plot(upper_threshold,1,'rx', label='Límite valores atípicos') #added upper outlier
plt.plot(lower_threshold,1,'rx') #added lower outlier
plt.legend(loc='upper right')
plt.grid()
plt.show()

#@title **Selección de 3 características limpio y filtrado**
data = np.array(df, dtype=float)
y_ = df_cl_filtrado.iloc[:,-1]
x_ = [df_cl_filtrado.iloc[:,0], df_cl_filtrado.iloc[:,1], df_cl_filtrado.iloc[:,2]]
ly = 'If'
lx = ['PF','e','dIf']

plt.figure(figsize=(15, 15))
plt.suptitle("Selección de 3 características")
for i in range(3):
  ax = plt.subplot(3, 3, i + 1)
  plt.plot(x_[i], y_, 'b.')
  plt.xlabel(lx[i],fontsize=15)
  plt.ylabel(ly,fontsize=15)
  plt.grid()
plt.subplots_adjust(wspace=0.3, hspace=0.5)
plt.show()

"""# **Regresor Lineal Multiparamétrico**

## **Regresor Lineal Multiparamétrico Personalizada**
"""

# Variables para almacenar la media y la desviación estándar de cada característica
mu = []
std = []

def load_data(df):
  data = np.array(df, dtype=float)
  normalize(data)
  return data[:,:-1], data[:,-1]


def normalize(data):
  for i in range(0,data.shape[1]):
    data[:,i] = (data[:,i] - data[:,i].min(axis=0)) / (data[:,i].max(axis=0) - data[:,i].min(axis=0))
    mu.append(np.mean(data[:,i]))
    std.append(np.std(data[:,i]))


def h(x,theta):
  return np.matmul(x, theta)

def cost_function(x, y, theta):
  return ((h(x, theta)-y).T@(h(x, theta)-y))/(2*y.shape[0])

def gradient_descent(x, y, theta, learning_rate=0.1, num_epochs=10):
  m = x.shape[0]
  J_all = []

  for _ in range(num_epochs):
    h_x = h(x, theta)
    cost_ = (1/m)*(x.T@(h_x - y))
    theta = theta - (learning_rate)*cost_
    J_all.append(cost_function(x, y, theta))

  return theta, J_all

def plot_cost(J_all, num_epochs):
  plt.figure(figsize = (8,5))
  plt.xlabel('Epocas',fontsize=15)
  plt.ylabel('Perdida',fontsize=15)
  plt.plot(num_epochs, J_all, 'm', linewidth = "5")
  plt.grid()
  plt.show()

def test(theta, x):
  x[0] = (x[0] - mu[0])/std[0]
  x[1] = (x[1] - mu[1])/std[1]
  x[2] = (x[2] - mu[2])/std[2]

  y = theta[0] + theta[1]*x[0] + theta[2]*x[1] + theta[3]*x[2]
  return y

x,y = load_data(df_cl_filtrado)
y = np.reshape(y, (len(y),1))
x = np.hstack((np.ones((x.shape[0],1)), x))
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

#@title **Entrenamiento**
theta = np.zeros((x.shape[1], 1))
learning_rate = 0.1
num_epochs = 20
theta, J_all = gradient_descent(x_train, y_train, theta, learning_rate, num_epochs)
J = cost_function(x_train, y_train, theta)
print("Cost: ", J)
print("Parameters: ", theta)

#@title **Cálculo y gráfico de perdida**
n_epochs_G = []
jplot_G = []
count_G = 0
start = time.perf_counter()
for i in J_all:
	jplot_G.append(i[0][0])
	n_epochs_G.append(count_G)
	count_G += 1
jplot_G = np.array(jplot_G)
n_epochs_G = np.array(n_epochs_G)
plot_cost(jplot_G, n_epochs_G)
end = time.perf_counter()
time_train_custom = (end-start)
print(f"Ejecución del tiempo {time_train_custom:.06f} secs.")

#@title **Predicción**
y_pred_G = []
start = time.perf_counter()
for i in range(len(x_test)):
  test_G = test(theta, x_test[i,1:len(x_test)])
  y_pred_G.append(test_G)
end = time.perf_counter()
time_pred_custom = (end-start)

y_pred_G = np.array(y_pred_G)
y_pred_G = y_pred_G.reshape(len(x_test),)
y_test = y_test.reshape(len(x_test),)

predict_list_G = {'Actual Value': y_test,
                'Predicted Value': y_pred_G,
                'Difference': abs(y_test-y_pred_G)}
df_predict_list_G = pd.DataFrame(predict_list_G)
df_predict_list_G

"""### **Línea de tendencia**"""

plt.figure(figsize=(8,5))
plt.xlabel('e',fontsize=15)
plt.ylabel('If',fontsize=15)
plt.scatter(x_test[:,3], y_test, color ='b')
plt.plot([x_test[:,3].min(), x_test[:,3].max()], [y_pred_G.min(), y_pred_G.max()], 'r--', lw=4,label='Línea de Tendencia')
plt.legend()
plt.grid()
plt.show()

#@title **Línea de tendencia para las 3 características**
data = np.array(df, dtype=float)
y_ = y_test
x_ = [x_test[:,1], x_test[:,2], x_test[:,3]]
ly = 'If'
lx = ['PF','e','dIf']

plt.figure(figsize=(15, 15))
plt.suptitle("Línea de tendencia para las 3 características")
for i in range(3):
  ax = plt.subplot(3, 3, i + 1)
  plt.plot(x_[i], y_, 'b.')
  plt.plot([x_[i].min(), x_[i].max()], [y_pred_G.min(), y_pred_G.max()], 'r--', lw=4,label='Línea de Tendencia')
  plt.xlabel(lx[i],fontsize=15)
  plt.ylabel(ly,fontsize=15)
  plt.grid()
plt.subplots_adjust(wspace=0.3, hspace=0.5)
plt.show()

plt.plot((y_test), 'b', label='Deseada')
plt.plot((y_pred_G), 'r--', label='Predicción')
plt.legend(fontsize=10)
plt.grid()
plt.show()

#@title **Metricas Modelo Personalizado**
mse_custom = mean_squared_error(y_test,y_pred_G)
rmse_custom = np.sqrt(mean_squared_error(y_test,y_pred_G))
r2_custom = r2_score(y_test,y_pred_G)

print("-- Para Modelo Personalizado --")
print("MSE:",mse_custom)
print("RMSE:",rmse_custom)
print("R2 score:",r2_custom)

"""## **Scikit-Learn**"""

#@title **Cargar Datos - scikitlearn**
x,y = load_data(df_cl_filtrado)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

x.shape

#@title **Entrenamiento**
regr1 = LinearRegression()

start = time.perf_counter()
regr1.fit(x_train,y_train)
end = time.perf_counter()
time_train_scikitlearn = (end-start)

coef = regr1.coef_
accuracy = regr1.score(x_test, y_test)
inter = regr1.intercept_

print('Coeficiente/Pendiente:',coef)
print('Coeficiente de determinación:',accuracy)
print('Intercepto:',inter)
print(f'Time Execution {time_train_scikitlearn:.06f} secs.')

coeficientes = regr1.coef_
intercepto = regr1.intercept_

# Imprimir la ecuación del regresor lineal
print("Ecuación del regresor lineal:")
print(f"y = {intercepto} + {coeficientes[0]} * x")

#@title **Predicción**
start = time.perf_counter()
predict = regr1.predict(x_test)
end = time.perf_counter()
time_pred_scikitlearn = (end-start)
cost = np.sqrt(mean_squared_error(y_test,predict))  # Root Mean Squared Error (RMSE)

print('Predicción 5 primeros valores:',predict[0:5])
print('Perdida:',cost)

#@title **Línea de Tendencia**
plt.figure(figsize = (8,5))
plt.xlabel('e',fontsize=15)
plt.ylabel('If',fontsize=15)
plt.scatter(x_test[:,2], y_test, color ='b')
plt.plot([x_test[:,2].min(), x_test[:,2].max()], [predict.min(), predict.max()], 'r--', lw=4,label='Línea de Tendencia')
plt.grid()
plt.legend(fontsize=15)
plt.show()

#@title **Línea de tendencia para las 3 características**
data = np.array(df, dtype=float)
y_ = y_test
x_ = [x_test[:,0], x_test[:,1], x_test[:,2]]
ly = 'If'
lx = ['PF','e','dIf']

plt.figure(figsize=(15, 15))
plt.suptitle("Línea de tendencia para las 3 características")
for i in range(3):
  ax = plt.subplot(3, 3, i + 1)
  plt.plot(x_[i], y_, 'b.')
  plt.plot([x_[i].min(), x_[i].max()], [predict.min(), predict.max()], 'r--', lw=4,label='Línea de Tendencia')
  plt.xlabel(lx[i],fontsize=15)
  plt.ylabel(ly,fontsize=15)
  plt.grid()
plt.subplots_adjust(wspace=0.3, hspace=0.5)
plt.show()

plt.plot((y_test), 'b', label='Deseada')
plt.plot((predict), 'r--', label='Predicción')
plt.legend(fontsize=10)
plt.grid()
plt.show()

#@title **Metricas Modelo scikitlearn**
mse_scikitlearn = mean_squared_error(y_test,predict)
rmse_scikitlearn = np.sqrt(mean_squared_error(y_test,predict))
r2_scikitlearn = r2_score(y_test,predict)

print("-- Para Modelo Personalizado --")
print("MSE:",mse_scikitlearn)
print("RMSE:",rmse_scikitlearn)
print("R2 score:",r2_scikitlearn)

"""# **Regresor polinómico**"""

df_cl_filtrado.head()

#@title **Cargar Datos - scikitlearn**
x,y = load_data(df_cl_filtrado)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

#@title **Entrenamiento**
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge
import numpy as np

# Genera datos de ejemplo
X = x_train  # Características
y = y_train  # Etiquetas

# Crear un pipeline con transformación polinómica y un estimador de regresión
pipeline = Pipeline([
    ('poly_features', PolynomialFeatures(degree=2)),  # Transformación polinómica de grado 2
    ('regressor', Ridge(alpha=1.0))  # Estimador de regresión (Ridge en este caso)
])

# Ajustar (fit) el pipeline a los datos
start = time.perf_counter()
pipeline.fit(X, y)
end = time.perf_counter()
time_train_scikitlearn_p = (end-start)

# Ahora el modelo está entrenado y listo para hacer predicciones.

#@title **Predicción**
coeficientes = pipeline.named_steps['regressor'].coef_

# Obtiene el término independiente (intercepto)
intercepto = pipeline.named_steps['regressor'].intercept_

# Imprime los coeficientes y el intercepto
print("Coeficientes:", coeficientes)
print("Intercepto:", intercepto)

# Construye la ecuación polinómica manualmente
grado = 2  # El grado del polinomio
ecuacion = f"y = {intercepto:.4f}"

for i, coef in enumerate(coeficientes):
    ecuacion += f" + {coef:.4f} * x^{i}"

print("Ecuación polinómica:", ecuacion)

#@title **Línea de Tendencia**
x = x_test[:,0]
y = 0.0973 + 0.0000 * x**0 + -0.0297 * x**1 + 0.0297 * x**2 + 0.4327 * x**3 + -0.0415 * x**4 + 0.0118 * x**5 + 0.2684 * x**6 + 0.0180 * x**7 + 0.1643 * x**8 + 0.2727 * x**9

plt.figure(figsize = (8,5))
plt.xlabel('e',fontsize=15)
plt.ylabel('If',fontsize=15)
plt.scatter(x_test[:,0], y_test, color ='b',label='Lineal')
plt.plot(x, y, 'r.', lw=4,label='Línea de Tendencia')
plt.grid()
plt.legend(fontsize=15)
plt.show()

#@title **Línea de tendencia para las 3 características**
data = np.array(df, dtype=float)
y_ = y_test
y = 0.0973 + 0.0000 * x**0 + -0.0297 * x**1 + 0.0297 * x**2 + 0.4327 * x**3 + -0.0415 * x**4 + 0.0118 * x**5 + 0.2684 * x**6 + 0.0180 * x**7 + 0.1643 * x**8 + 0.2727 * x**9
x_ = [x_test[:,0], x_test[:,1], x_test[:,2]]
ly = 'If'
lx = ['PF','e','dIf']

plt.figure(figsize=(15, 15))
plt.suptitle("Línea de tendencia para las 3 características")
for i in range(3):
  ax = plt.subplot(3, 3, i + 1)
  plt.plot(x_[i], y_, 'b.')
  plt.plot(x, y, 'r.', lw=4,label='Línea de Tendencia')
  plt.xlabel(lx[i],fontsize=15)
  plt.ylabel(ly,fontsize=15)
  plt.grid()
plt.subplots_adjust(wspace=0.3, hspace=0.5)
plt.show()

#@title **Predicción**
start = time.perf_counter()
predicciones = pipeline.predict(x_test)
end = time.perf_counter()
time_pred_scikitlearn_p = (end-start)
plt.plot((y_test), 'b', label='Deseada')
plt.plot((predicciones), 'r--', label='Predicción')
plt.legend(fontsize=10)
plt.grid()
plt.show()

#@title **Metricas Modelo scikitlearn polinómico**
mse_scikitlearn_p = mean_squared_error(y_test,predicciones)
rmse_scikitlearn_p = np.sqrt(mean_squared_error(y_test,predicciones))
r2_scikitlearn_p = r2_score(y_test,predicciones)

print("-- Para Modelo Personalizado --")
print("MSE:",mse_scikitlearn_p)
print("RMSE:",rmse_scikitlearn_p)
print("R2 score:",r2_scikitlearn_p)

"""#**Descripción de resultados**"""

#@title **Descripción general**
results = {'Method/Optimizer': pd.Series(['RL-Personalizado','RL-scikitlearn','RL-scikitlearn_p']),
             'MSE': pd.Series([mse_custom, mse_scikitlearn, mse_scikitlearn_p]),
             'RMSE': pd.Series([rmse_custom, rmse_scikitlearn, rmse_scikitlearn_p]),
             'R2': pd.Series([r2_custom, r2_scikitlearn, r2_scikitlearn_p]),
             'Train execution Time': pd.Series([time_train_custom, time_train_scikitlearn, time_train_scikitlearn_p]),
           'Prediction execution Time': pd.Series([time_pred_custom, time_pred_scikitlearn, time_pred_scikitlearn_p])
            }
df_results = pd.DataFrame(results)
df_results = df_results.sort_values('RMSE',ascending=True)
df_results